# ğŸ¦ Bank Marketing Campaign Prediction using Machine Learning

![Python](https://img.shields.io/badge/Python-3.9-blue)
![Machine Learning](https://img.shields.io/badge/Task-Classification-orange)
![Status](https://img.shields.io/badge/Project-Completed-success)

---

## ğŸ“Œ Project Overview

This project focuses on predicting whether a customer will subscribe to a bank term deposit using Machine Learning classification models.
The work was completed as part of a **Data Science & Machine Learning Internship** at **United Network of Professionals (UNP)**.

The repository demonstrates an end-to-end machine learning workflow including data preprocessing, feature engineering, model comparison, and performance evaluation.

---

## ğŸ¯ Objectives

* Perform Exploratory Data Analysis (EDA)
* Handle categorical variables using encoding techniques
* Apply feature selection using Recursive Feature Elimination (RFE)
* Train multiple classification models
* Compare performance using accuracy metrics
* Identify the best predictive model

---

## ğŸ§  Machine Learning Workflow

1. Data Cleaning & Preprocessing
2. Exploratory Data Analysis (EDA)
3. Feature Engineering & Encoding
4. SMOTE Oversampling for Imbalance Handling
5. Model Training & Evaluation
6. Performance Comparison

---

## ğŸ¤– Models Implemented

* Logistic Regression (RFE Selection)
* Logistic Regression (All Features)
* Support Vector Machine (SVM)
* K-Nearest Neighbours (KNN)
* Decision Tree Classifier
* Random Forest Classifier
* Bagging Classifier
* AdaBoost Classifier
* Gradient Boosting Classifier
* XGBoost Classifier
* Neural Network (TensorFlow)

---

## ğŸ“Š Model Performance Comparison (Accuracy)

| Model                          | Accuracy (%) |
| ------------------------------ | ------------ |
| Logistic Regression (RFE)      | 91.10        |
| Logistic Regression (All Data) | 90.59        |
| Support Vector Machine (SVM)   | 91.30        |
| K-Nearest Neighbours (KNN)     | 91.35        |
| XGBoost Classifier             | **92.33** ğŸ† |
| Gradient Boosting              | 92.30        |
| AdaBoost                       | 91.01        |
| Bagging Classifier             | 91.25        |
| Random Forest                  | 91.28        |
| Decision Tree                  | 90.85        |
| Neural Network (TensorFlow)    | 88.54        |

> ğŸ† **Best Performing Model:** XGBoost Classifier â€” Accuracy **92.33%**

ğŸ“Œ Ensemble learning methods achieved the highest performance, indicating strong non-linear relationships within the dataset.

---

## ğŸ“‚ Repository Structure

```
Bank_Final
â”‚
â”œâ”€â”€ Bank_Final.ipynb
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Tools & Libraries

* Python
* Pandas & NumPy
* Scikit-learn
* TensorFlow / Keras
* XGBoost
* Matplotlib & Seaborn
* Statsmodels

---

## ğŸš€ How to Run the Project

Install dependencies:

```
pip install -r requirements.txt
```

Open notebook:

```
Bank_Final.ipynb
```

---

## ğŸ“ˆ Key Highlights

âœ” Real-world internship dataset
âœ” Feature Selection using RFE
âœ” SMOTE for class imbalance handling
âœ” Ensemble learning & deep learning models
âœ” Comparative performance evaluation

---

## ğŸ‘¨â€ğŸ’» Author

**Aditya Charan Eranki**
Machine Learning | Data Analytics | Data Science Enthusiast
